[2023-10-11 12:22:37,897] {taskinstance.py:876} INFO - Dependencies all met for <TaskInstance: job_posting_aggregator.scrape_linkedin 2023-10-11T12:22:36.487359+00:00 [queued]>
[2023-10-11 12:22:37,913] {taskinstance.py:876} INFO - Dependencies all met for <TaskInstance: job_posting_aggregator.scrape_linkedin 2023-10-11T12:22:36.487359+00:00 [queued]>
[2023-10-11 12:22:37,915] {taskinstance.py:1067} INFO - 
--------------------------------------------------------------------------------
[2023-10-11 12:22:37,917] {taskinstance.py:1068} INFO - Starting attempt 1 of 2
[2023-10-11 12:22:37,922] {taskinstance.py:1069} INFO - 
--------------------------------------------------------------------------------
[2023-10-11 12:22:37,943] {taskinstance.py:1087} INFO - Executing <Task(PythonOperator): scrape_linkedin> on 2023-10-11T12:22:36.487359+00:00
[2023-10-11 12:22:37,953] {standard_task_runner.py:52} INFO - Started process 443 to run task
[2023-10-11 12:22:37,962] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'job_posting_aggregator', 'scrape_linkedin', '2023-10-11T12:22:36.487359+00:00', '--job-id', '25', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/get_jobs_data.py', '--cfg-path', '/tmp/tmps91gvuaf', '--error-file', '/tmp/tmpdu_yh8ol']
[2023-10-11 12:22:37,964] {standard_task_runner.py:77} INFO - Job 25: Subtask scrape_linkedin
[2023-10-11 12:22:38,036] {logging_mixin.py:104} INFO - Running <TaskInstance: job_posting_aggregator.scrape_linkedin 2023-10-11T12:22:36.487359+00:00 [running]> on host 62cfcfb44bb5
[2023-10-11 12:22:38,116] {taskinstance.py:1282} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=you
AIRFLOW_CTX_DAG_ID=job_posting_aggregator
AIRFLOW_CTX_TASK_ID=scrape_linkedin
AIRFLOW_CTX_EXECUTION_DATE=2023-10-11T12:22:36.487359+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-10-11T12:22:36.487359+00:00
[2023-10-11 12:22:43,875] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3704998318
[2023-10-11 12:22:43,920] {get_jobs_data.py:124} INFO - publish_date: 1 month ago
[2023-10-11 12:22:44,690] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3705729043
[2023-10-11 12:22:44,712] {get_jobs_data.py:124} INFO - publish_date: 1 month ago
[2023-10-11 12:22:44,885] {get_jobs_data.py:179} INFO - 2023-10-11T12:22:44.885119
[2023-10-11 12:22:44,899] {get_jobs_data.py:179} INFO - Berlin, Germany
[2023-10-11 12:22:44,902] {get_jobs_data.py:179} INFO - Data Engineer
[2023-10-11 12:22:49,805] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3634010998
[2023-10-11 12:22:49,808] {get_jobs_data.py:124} INFO - publish_date: 3 months ago
[2023-10-11 12:22:50,218] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3700412243
[2023-10-11 12:22:50,220] {get_jobs_data.py:124} INFO - publish_date: None
[2023-10-11 12:22:50,223] {get_jobs_data.py:179} INFO - 2023-10-11T12:22:50.223147
[2023-10-11 12:22:50,224] {get_jobs_data.py:179} INFO - Munich, Germany
[2023-10-11 12:22:50,226] {get_jobs_data.py:179} INFO - Data Engineer
[2023-10-11 12:22:55,628] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3704998318
[2023-10-11 12:22:55,633] {get_jobs_data.py:124} INFO - publish_date: 1 month ago
[2023-10-11 12:22:56,272] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3705729043
[2023-10-11 12:22:56,279] {get_jobs_data.py:124} INFO - publish_date: 1 month ago
[2023-10-11 12:22:56,306] {get_jobs_data.py:179} INFO - 2023-10-11T12:22:56.306444
[2023-10-11 12:22:56,309] {get_jobs_data.py:179} INFO - Berlin, Germany
[2023-10-11 12:22:56,312] {get_jobs_data.py:179} INFO - Big Data Engineer
[2023-10-11 12:23:00,505] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3700418398
[2023-10-11 12:23:00,508] {get_jobs_data.py:124} INFO - publish_date: 1 month ago
[2023-10-11 12:23:00,921] {get_jobs_data.py:116} INFO - https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/3634010998
[2023-10-11 12:23:00,926] {get_jobs_data.py:124} INFO - publish_date: None
[2023-10-11 12:23:00,928] {get_jobs_data.py:179} INFO - 2023-10-11T12:23:00.928176
[2023-10-11 12:23:00,929] {get_jobs_data.py:179} INFO - Munich, Germany
[2023-10-11 12:23:00,931] {get_jobs_data.py:179} INFO - Big Data Engineer
[2023-10-11 12:23:00,939] {python.py:151} INFO - Done. Returned value was: None
[2023-10-11 12:23:00,954] {taskinstance.py:1191} INFO - Marking task as SUCCESS. dag_id=job_posting_aggregator, task_id=scrape_linkedin, execution_date=20231011T122236, start_date=20231011T122237, end_date=20231011T122300
[2023-10-11 12:23:00,986] {taskinstance.py:1245} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-10-11 12:23:01,029] {local_task_job.py:151} INFO - Task exited with return code 0
